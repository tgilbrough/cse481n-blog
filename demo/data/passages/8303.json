[{"url": "https://developer.yahoo.com/hadoop/tutorial/module2.html", "passage_text": "Step 2: Copy a file from HDFS to the local file system. The get command is the inverse operation of put; it will copy a file or directory (recursively) from HDFS into the target of your choosing on the local file system. A synonymous operation is called-copyToLocal."}, {"url": "http://www.michael-noll.com/blog/2011/10/20/understanding-hdfs-quotas-and-hadoop-fs-and-fsck-tools/", "passage_text": "In my experience Hadoop users often confuse the file size numbers reported by commands such as hadoop fsck, hadoop fs-dus and hadoop fs-count-q when it comes to reasoning about HDFS space quotas. Here is a short summary how the various filesystem tools in Hadoop work in unison."}, {"url": "http://blog.cloudera.com/blog/2009/02/the-small-files-problem/", "passage_text": "Problems with small files and HDFS. A small file is one which is significantly smaller than the HDFS block size (default 64MB). If you\u2019re storing small files, then you probably have lots of them (otherwise you wouldn\u2019t turn to Hadoop), and the problem is that HDFS can\u2019t handle lots of files."}, {"url": "http://stackoverflow.com/questions/11368907/hadoop-block-size-and-file-size-issue", "passage_text": "You see that each file size is lesser than the block size which is 128 MB. These files are in KB. HDFS capacity is consumed based on the actual file size but a block is consumed per file. There are limited number of blocks available dependent on the capacity of the HDFS."}, {"url": "http://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/", "passage_text": "In this tutorial we will walk through some of the basic HDFS commands you will need to manage files on HDFS. To complete this tutorial you will need a working HDP cluster. The easiest way to have a Hadoop cluster is to download the Hortonworks Sandbox."}, {"url": "http://blog.mgm-tp.com/2010/04/hadoop-log-management-part2/", "passage_text": "1 Hadoop Distributed Filesystem (HDFS): This is a file system which spans over all servers in the Hadoop cluster and provides a single namespace for storing files. 2  It is optimized for reading and writing large files. 3  The files are broken down into blocks with a size of at least 64 MB."}, {"url": "http://serverfault.com/questions/300135/hadoop-hdfs-set-file-block-size-from-commandline", "passage_text": "You can do this by setting-Ddfs.block.size=something with your hadoop fs command. For example: As you can see here, the block size changes to what you define on the command line (in my case, the default is 64MB, but I'm changing it down to 1MB here)."}]
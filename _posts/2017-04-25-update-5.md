---
layout: post
title: Multiple Baseline Models' Performance and Error Analysis
comments: true
---

### [](#header-3)Baseline Model (Old)

For our baseline model, we started with a very basic design. The model has two bi-directional dynamic GRU RNN's. The first RNN takes a matrix of the word embeddings of the question, and the second RNN takes the word embeddings of the context. The output of each RNN is the concatenation of the backward and forward outputs of that RNN. The output of the question RNN is a matrix representing a vector for each word in the question. The question representation we later consider is an average of the different word vectors outputed from the question RNN. The question representation and the output of the context RNN are concatenated and inserted in a third bi-directional dynamic GRU RNN. Then, we have a dense output layer that takes the output of the third RNN. From the output layer we get the logits for the starting and ending indices of the answer, from which we take the arg max. All three RNN's have a dropout as a regularization technique to reduce overfitting during training.

![Overview of the model from Tensorboard]({{ site.baseurl }}/img/post-5/baseline_graph_1.JPG)
![The expansion of the question encoding block]({{ site.baseurl }}/img/post-5/baseline_graph_2.JPG)
![The expansion of the context encoding block]({{ site.baseurl }}/img/post-5/baseline_graph_3.JPG)
![The expansion of the post process block]({{ site.baseurl }}/img/post-5/baseline_graph_4.JPG)

[baseline_model.py](https://github.com/tgilbrough/MrKnowItAll/blob/master/baseline_model.py)

### [](#header-3)Baseline Attention Model (New)

The attention model is very similar to the old baseline model. The only difference is in defining the question representation that we use in the third RNN. Before, we used to average the difference word vectors we're getting from the question RNN, we do a weighted average with trained weights.

![Overview of the model from Tensorboard]({{ site.baseurl }}/img/post-5/attention_graph_1.JPG)
![The expansion of the encoding block]({{ site.baseurl }}/img/post-5/attention_graph_2.JPG)
![The expansion of the attention block]({{ site.baseurl }}/img/post-5/attention_graph_3.JPG)
![The expansion of the post process block]({{ site.baseurl }}/img/post-5/attention_graph_4.JPG)

[attention_model.py](https://github.com/tgilbrough/MrKnowItAll/blob/master/attention_model.py)

### [](#header-3)Initial Hyperparameter Tuning

These baseline models have multiple hyperparameters to tune, all of which can be controlled through the command line interface. So far, our list includes:

- **Keep Probability** - Used to configure all dropout layers within the model
- **Embeddings Size** Attention - Can be 50, 100, 200, or 300. Chooses which pretrained embeddings to load
- **Learning Rate** - Used as the hyperparameter within the model optimizer
- **Hidden Size** - Dimensions of hidden layers within RNN cells for encoding and decoding data
- **Epochs** - Number of passes over the training data
- **Batch Size** - Number of training samples to feed into the system at once, greatly effects runtime on GPU

In addition to these hyperparameters, the command line interface can also configure which model to use as well as which question type. With all this, we decided to run a couple of experiments in order to get a rough idea of what makes good default values for these values. We captured the results in Tensorboard, which are displayed below. As a note, the overall loss is the sum of the loss for the starting and ending index losses. Those individual losses are measuring exact matches of those indices.

For these experiments, we trained and tested on the location question type, for 50 epochs, with an embedding size of 50. We varied the batch size based on the model, since using too large of a batch size caused OOM (Out of Memory) errors. Therefore we will analyze the effect of different keep probabilities, learning rates, and hidden sizes.

(Sorry for the lack of a color key, was cut off by the browser but can follow which line is which with the values)

#### [](#header-4)Keep Probability
Holding the hidden size at 50 and learning rate at 0.01, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline
![Baseline Keep Prob]({{ site.baseurl }}/img/post-5/baseline_keepprob.png)

##### [](#header-5)Baseline Attention
![Attention Keep Prob]({{ site.baseurl }}/img/post-5/attention_keepprob.png)

Clearly, the keep probability of 0.7 was prone to overfitting and performed the worst on both models after 50 epochs. But within these first 50 epochs, the 0.3 and 0.5 keep probabilities performed comparably. If we extend the training longer, we hypothesis that the 0.3 value will eventually work better.

#### [](#header-4)Learning Rate
Holding the hidden size at 50 and keep probability at 0.5, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline
![Baseline Learning Rate]({{ site.baseurl }}/img/post-5/baseline_learningrate.png)

##### [](#header-5)Baseline Attention
![Attention Learning Rate]({{ site.baseurl }}/img/post-5/attention_learningrate.png)

Just by viewing the performance of the models above, it is quite obvious that the 0.01 learning rate performed the best. In fact, the 0.5 rate would not even work with the attention model since it ran into Tensorflow issues with NaN values. Moving forward, we will choose to use 0.01 as a default rate and do further tuning to the hyperparameter once we have a more advanced model.

#### [](#header-4)Hidden Size
Holding the learning rate at 0.01 and keep probability at 0.5, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline
![Baseline Hidden Size]({{ site.baseurl }}/img/post-5/baseline_hiddensize.png)

##### [](#header-5)Baseline Attention
![Attention Hidden Size]({{ site.baseurl }}/img/post-5/attention_hiddensize.png)

The differences in performance for the varying hidden sizes are much smaller than with the other hyperparameters we tested. But yet, we still view the trend that the smaller this value is, the better with these baseline models. Using the value of 50, for now, was a nice balance between having the representative power to learn patterns, while not being too large and overfitting the training data.

### [](#header-3)Performance of Attention vs. Non-Attention Model

### [](#header-3)Error Analysis

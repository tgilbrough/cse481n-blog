---
layout: post
title: Formal Project Proposal
comments: true
hidden: true
---

(INSERT BRIEF ABSTRACT HERE OF PROJECT)

### [](#header-3)Project Objectives

* * *

### [](#header-3)Proposed Methodologies

Below we outline a rough plan in order to provide checkpoints that display tangible progress with a RNN model trained and used on the the MS MARCO dataset.

1.  Preprocess data, spliting it into 5 sets, one for each of the answer types within the MS MARCO dataset.
2.  Choose one of the answer types to start training a model with. This is in order to reduce the time spent training the model, and to get a baseline implementation working.
3.  Implement a simple RNN, either LSTM or GRU, that can take in passages, read the question, and produce the location of the answer within the passages.
4.  Add an attention mechanism to the RNN.
5.  Experiement with more advanced attention mechanisms and RNN architectures. 
6.  Experiement with exteneding the model to multiple of the answer types. This may involve transfer learning or training an initial model or all answer types, and composing it with trained model made specifically with each answer type.
7.  Begin to synthesize multiple pieces of an answer into a single, succinct answer.  

* * *

### [](#header-3)Avaiable Resources

*   Software Packages
    *   Tensorflow
    *   word2vec
    *   GloVe
    *   The Stanford Parser
    *   NLTK
*   Papers
    *   [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/pdf/1611.09268v2.pdf)
    *   [Machine Comprehension Using Match-LSTM and Answer Pointer](https://arxiv.org/pdf/1608.07905.pdf)
    *   [ReasoNet: Learning to Stop Reading in Machine Comprehension](https://arxiv.org/pdf/1609.05284.pdf)

* * *

### [](#header-3)Evaluation Plan

Luckily, Microsoft has provided scripts on the MS MARCO submission page that we will use to evaluate our model. We may have to edit the scripts if we only end up using a subset of the MS MARCO dataset. The produced Bleu-1 and Rouge-L will still be comparable to numbers on their leaderboard, with the exception that ours may only be based on a subset of the dataset versus it in its entirety. 

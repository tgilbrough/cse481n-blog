---
layout: post
title: Multiple Baseline Models' Performance and Error Analysis
comments: true
hidden: true
---

### [](#header-3)Baseline Attention Model

### [](#header-3)Initial Hyperparameter Tuning

These baseline models have multiple hyperparameters to tune, all of which can be controlled through the command line interface. So far, our list includes:

- **Keep Probability** - Used to configure all dropout layers within the model
- **Embeddings Size** - Can be 50, 100, 200, or 300. Chooses which pretrained embeddings to load
- **Learning Rate** - Used as the hyperparameter within the model optimizer
- **Hidden Size** - Dimensions of hidden layers within RNN cells for encoding and decoding data
- **Epochs** - Number of passes over the training data
- **Batch Size** - Number of training samples to feed into the system at once, greatly effects runtime on GPU

In addition to these hyperparameters, the command line interface can also configure which model to use as well as which question type. With all this, we decided to run a couple of experiments in order to get a rough idea of what makes good default values for these values. We captured the results in Tensorboard, which are displayed below. As a note, the overall loss is the sum of the loss for the starting and ending index losses. Those indivudal losses are measuring exact matches of those indices.

For these experiments, we trained and tested on the location question type, for 50 epochs, with an embedding size of 50. We varied the batch size based on the model, since using too large of a batch size casued OOM (Out of Memory) errors. Therefore we will analyze the effect of different keep probabilities, learning rates, and hidden sizes.


![Baseline Keep Prob]({{ site.url }}/img/baseline_keepporb.png) 
![Attention Keep Prob]({{ site.url }}/img/attention_keepporb.png) 

![Baseline Learning Rate]({{ site.url }}/img/baseline_learningrate.png) 
![Attention Learning Rate]({{ site.url }}/img/attention_learningrate.png) 

![Baseline Hidden Size]({{ site.url }}/img/baseline_hiddensize.png) 
![Attention Hidden Size]({{ site.url }}/img/attention_hiddensize.png) 


### [](#header-3)Performance of Attention vs. Non-Attention Model

### Error Analysis


---
layout: post
title: Multi-passage Model Update
comments: true
hidden: true
---

### [](#header-3)Completed BiDAF
(Maybe talk about how it was more complex than the diagram implied)


### [](#header-3)Introduced Multiple Passages
We figured with time running short, it was time to start to incorporate multiple passages into the model. In our eyes, there were two main ways of doing this, first, feed all the passages into the model and have some sort of attention mechanism find the relevant passages. The second option would be to calculate a probability of how relevant the passage was, then feed each passage through the model, take the probability distribution of the start and end index over the words, multiple the probabilities by the probability that the passage is relevant, and finally taking an argmax to find the answer.

For this first attempt at the multi-passage model, we decided to go with the first option with advice from Maarten. But instead of having a set of learned attention waits, we took the cosine similarities of the tf-idf vectors mentioned last week, and performed a softmax over them to get probability distribution of which passages are relevant. From there, we apply these weights to the output vectors from the 'Context Embed Layer' viewed below.

![BiDAF Diagram]({{ site.baseurl }}/img/post-8/BiDAF.jpg)<br/>

Since this was our first attempt, we stuck with just concatenating the inputs together to make one long string. In future attempts, it may be beneficial to have the LSTM in the 'Context Embed Layer' read only single passages at a time, so it does not need to learn to embed context from multiple different passages at once.

Running the model for 50 epochs, with an embedding and hidden size of 50, and a learning rate of 0.01, we got the following results:

| Metric  | BiDAF-Multi |
| ------- | ------------------ |
| Bleu-1  | 0.101 |
| Bleu-2  | 0.081 |
| Bleu-3  | 0.070 |
| Bleu-4  | 0.062 |
| Rouge-L | 0.291 |

### [](#header-3)Error Analysis

### [](#header-3)How to Improve
Needless to say, these results are pretty disappointing. We fully expected a dip in our performance since we are now sending in much more text into the model, some relevant and some not. The very low numbers can be due to a number of reasons, including a possible bug in our implementation. Looking forward to what to do next to fix the model, our first step was contacting Minjoon Seo, one of the original authors of the BiDAF paper. We hope that we can possibly give us a few pointers on how to improve and maybe even where a bug may be present in the code.

Alternate approaches include the argmax model that we mentioned in the previous section, and also sorting the passages based on their relevance scores. This way the answer should lie closer to the beginning of the concatenated text, although, this should not be necessary for the model to succeed. One other possible change that would be easy would be apply the passage relevance weights elsewhere in the model. Without much prior knowledge of attention mechanisms, this would take some playing with.

### [](#header-3)Demo Work
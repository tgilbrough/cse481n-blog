---
layout: post
title: Advanced Model Update #1
comments: true
hidden: true
---

### [](#header-3)What We Are Trying
For our first advanced model, we decided to attempt to implement the model presented in Salesforce Research's paper, Dynamic Coattention Networks. The paper was very well written, with clear explanations, and was state-of-the-art on the SQuAD dataset was the paper came out. At this stage of the project, we are still trying to perfect the models ability to find an answer within the selected passage it is fed. 

As presented in the paper, there are two major components to this model:

- **Coattention Encoder** - An attention mechanism that accounts for the important words in the passage in light of the question, and the important words in the question in light of the passage. Inutiatively, this is like reading the question first and then searching the passage for the answer by looking for words that seem particularly relevant. 
- **Dynamic Decoder** - This is an iterative approach to decode an answer. The start and end index are in conversation with each other as they converge to a solution the model thinks is best. Within each iteration, the paper makes use of an LSTM, as well as a "highway maxout network" for each the start and end index.
 
### [](#header-3)What is Going Wrong
So far we have implemented the whole model according to the paper in Tensorflow. That being said, when we attempt to train the model, the loss function stagnates at the same value. Beginning beginners at deep learning, this may require a good deal of debugging. For the time being, we removed the decoder and replaced it with two dense layers with a single output each, for the start and end index. This was a small enough model for it to learn to reasonable and produce results comparable to our other models. Moving forward, we will have to focus on the decoder and see if it is wired incorrectly, or subject to another bug.

### [](#header-3)Results So Far

### [](#header-3)Next Steps

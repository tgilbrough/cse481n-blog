---
layout: post
title: Multi-passage Preliminary Results
comments: true
hidden: false
---


### [](#header-3)Multiple Passages Results

### [](#header-3)Error Analysis

The demo has made error analysis much easier, since we can now easily see the
relevance of passages as well as the start of answer and end of answer probabilities
for each word.

#### No Answer

Our model has a problem with occasionally predicting zero for both the start and
end indices, which yields no answer. This seems to happen with about 5% of questions,
which could have a significant impact on our Bleu and Rouge scores. It seems that
the model predicts no answers when the most likely end token appears before the most
likely start token. We should be able to fix this without too much trouble.

![No answer start]({{ site.baseurl }}/img/post-9/no-answer-start.png)<br/>
![No answer end]({{ site.baseurl }}/img/post-9/no-answer-end.png)<br/>

#### Poor Numeric Answers

Numeric answers represent about a quarter of the dataset. Our answers for numeric
questions aren't quite as good as they are for other answer types. I think that
this stems from the word embeddings that we are using. While some numbers are
representing in the GloVe embeddings, most are just treated as unknowns.
We could do something much smarter here. Simply using a special "number" symbol
would probably work better than unknown. We could also have a separate symbol
for each unit. This is low hanging fruit that could improve our numeric scores
substantially.

#### Lack of Comprehension

For some questions, our model provides plausible but incorrect answers. For the question
below, the true answer is "8-9 hours", but our answer is "7-9 hours". We predict a number
at least, but some of the nuance is not understood. Right now we have only seen the demo
on our attention model, but it would be interesting to see how other models do in
situations like this.

![Sleep]({{ site.baseurl }}/img/post-9/sleep.png)<br/>

### [](#header-3)Areas For Improve

### [](#header-3)Demo Work

A lot of our time in the past week had been devoted to our presentation and demo.
The demo is available [here](https://tgilbrough.github.io/cse481n-blog/demo/).
We ran our attention model on the MS MARCO testing dataset to produce answers
for the demo. Unfortunately, we could not extract answers for description questions
due to time constraints and bugs in our code, but we will have these for the final demo.

Right now our demo is limited to questions in the test dataset. Extending it to
free-form input would be very interesting, but outside of the scope of this project,
since it would require scraping Bing/Google and then extracting context passages
from webpages.


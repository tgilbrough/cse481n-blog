---
layout: post
title: Multi-passage Preliminary Results
comments: true
hidden: true
---


### [](#header-3)Multiple Passages Results
Last week we experimented with extending the BiDAF model to take in multiple passages, and have an attention mechanism across those passages. Unfortunately, we did not get the results we were hoping for. This week we went ahead and implemented an argmax approach. 

In more detail, we sent each passage through the answer extractor individually, and grabbed the probability of the most likely start and end indexes. We then multiplied them together along with the tf-idf similarity between that passage and the query. After doing this for all the passages, we choose the answer that had the highest product of the terms just described. 

One benefit of this approach over the larger models with attention over the passages is that we can train the answer extractor on it's own, and then switch them in and out of the argmax framework for testing. In addition, it is a much simpler model and therefore is easier to train.

Trying the argmax approach with all of our previous models on the location dataset, we get the following results:

|         | Baseline | Attention | Coattention (Bug) | BiDAF |
| ------- | -------- | --------- | ----------------- | ----- |
| Bleu-1  |   0.292  |   0.301   |       0.129       | 0.302 | 
| Rouge-L |   0.286  |   0.297   |       0.228       | 0.291 | 

Here are the results with the attention answer extractor on the different answer types:

|         | Description | Numeric | Entity | Location | Person |
| ------- | ----------- | ------- |------- | -------- | ------ |
| Bleu-1  |   0.298     |  0.215  | 0.201  |  0.301   |  0.240 | 
| Rouge-L |   0.298     |  0.330  | 0.205  |  0.297   |  0.287 | 



### [](#header-3)Error Analysis

### [](#header-3)Areas For Improve

With the exception of the coattention model, which we believe we introduced a bug to at some point, the results are much better than last week's. It was interesting that there was not much variation in results for the different answer extractors. This leads us to believe that we can benefit the most at this point by improving how we select the answers after they are fed through the answer extractor. This may mean weighting the passages differently, and accounting for the different lengths of passages. In addition to this, hyper-parameter tuning should be able to boost the results at least a bit as well.

### [](#header-3)Demo Work


---
layout: post
title: Multiple Baseline Models' Performance and Error Analysis
comments: true
---

### [](#header-3)Baseline Attention Model

### [](#header-3)Initial Hyperparameter Tuning

These baseline models have multiple hyperparameters to tune, all of which can be controlled through the command line interface. So far, our list includes:

- **Keep Probability** - Used to configure all dropout layers within the model
- **Embeddings Size** - Can be 50, 100, 200, or 300. Chooses which pretrained embeddings to load
- **Learning Rate** - Used as the hyperparameter within the model optimizer
- **Hidden Size** - Dimensions of hidden layers within RNN cells for encoding and decoding data
- **Epochs** - Number of passes over the training data
- **Batch Size** - Number of training samples to feed into the system at once, greatly effects runtime on GPU

In addition to these hyperparameters, the command line interface can also configure which model to use as well as which question type. With all this, we decided to run a couple of experiments in order to get a rough idea of what makes good default values for these values. We captured the results in Tensorboard, which are displayed below. As a note, the overall loss is the sum of the loss for the starting and ending index losses. Those individual losses are measuring exact matches of those indices.

For these experiments, we trained and tested on the location question type, for 50 epochs, with an embedding size of 50. We varied the batch size based on the model, since using too large of a batch size caused OOM (Out of Memory) errors. Therefore we will analyze the effect of different keep probabilities, learning rates, and hidden sizes.

(Sorry for the lack of a color key, was cut off by the browser but can follow which line is which with the values)

#### [](#header-4)Keep Probability
Holding the hidden size at 50 and learning rate at 0.01, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline 
![Baseline Keep Prob]({{ site.baseurl }}/img/baseline_keepprob.png) 

##### [](#header-5)Baseline Attention
![Attention Keep Prob]({{ site.baseurl }}/img/attention_keepprob.png)

Clearly, the keep probability of 0.7 was prone to overfitting and performed the worst on both models after 50 epochs. But within these first 50 epochs, the 0.3 and 0.5 keep probabilities performed comparably. If we extend the training longer, we hypothesis that the 0.3 value will eventually work better.

#### [](#header-4)Learning Rate
Holding the hidden size at 50 and keep probability at 0.5, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline 
![Baseline Learning Rate]({{ site.baseurl }}/img/baseline_learningrate.png) 

##### [](#header-5)Baseline Attention
![Attention Learning Rate]({{ site.baseurl }}/img/attention_learningrate.png) 

Just by viewing the performance of the models above, it is quite obvious that the 0.01 learning rate performed the best. In fact, the 0.5 rate would not even work with the attention model since it ran into Tensorflow issues with NaN values. Moving forward, we will choose to use 0.01 as a default rate and do further tuning to the hyperparameter once we have a more advanced model.

#### [](#header-4)Hidden Size
Holding the learning rate at 0.01 and keep probability at 0.5, we can observe the performance of the baseline and baseline attention model model:

##### [](#header-5)Baseline 
![Baseline Hidden Size]({{ site.baseurl }}/img/baseline_hiddensize.png) 

##### [](#header-5)Baseline Attention
![Attention Hidden Size]({{ site.baseurl }}/img/attention_hiddensize.png) 

The differences in performance for the varying hidden sizes are much smaller than with the other hyperparameters we tested. But yet, we still view the trend that the smaller this value is, the better with these baseline models. Using the value of 50, for now, was a nice balance between having the representative power to learn patterns, while not being too large and overfitting the training data.

### [](#header-3)Performance of Attention vs. Non-Attention Model

### [](#header-3)Error Analysis


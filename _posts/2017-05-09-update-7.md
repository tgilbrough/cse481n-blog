---
layout: post
title: Advanced Model Update #2 And Passage Relevance
comments: true
hidden: true
---

## [](#header-2)Answer Extraction

### [](#header-3)What We Are Trying


### [](#header-3)What is Going Wrong


### [](#header-3)Further Error Analysis


### [](#header-3)Next Steps

* * *

## [](#header-2)Passage Relevance

### [](#header-3)What We Are Trying
The biggest unanswered question we have right now in our model is how to handle passage relevance. For each sample, we are given an average of 8.21 passages averaging 421.68 characters. Sometimes only one of these passages are relevant, sometimes they all are. The dataset supplies an 'is_selected' attribute for each passage to tell if that passage was used to produce the answer. For each sample, an average of 1.07 passages are selected.

With all this, we have a few options of how to deal with passage relevance. The first question is do we concatinate the passages? We could concatenate all the passages or just a select few after another model filters out irrelevant passages. The issue with this is that we would have to train quite a large model and so training may be expense and have a tough time converging.

Another question to consider is do we use a model to pick which passages are relevant, and then send those passages in and not the others. An issue with this is that we would have to have a trustworthy passage selector to not throw out the answer along with the passage at this step. An alternative to this strategy that would allievate this worry would be to simply rank the passages. This way we do not discard any information, but are able to focus the model on certain passages more than other based on the output from a separately trained model, acting almost as an attention mechanism. This would require some sort of method of picking the best answer out of a group of weighted answers in the end, not just picking the answer from the highest ranked passage.

One final question we are considering is to go neural or not. With the issue of passage relevance, we could use almost the same model as the answer extractor, just by switching the output to produce a single relevance number instead of two indexes in the text. A much simplier approach would be to use a non-neural model, whether it uses hand crafted features or still uses the GloVe embedding somehow.

So for this first attempt at passage relevance, we tried building both a neural model, based on our baseline answer extractor, and a non-neural model. 

### [](#header-3)Preliminary Results 


### [](#header-3)Next Steps

